{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC: A Julia application\n",
    "\n",
    "## The Metropolis Hastings algorithm\n",
    "The MH returns samples from the posterior distribution\n",
    "$$\n",
    "p(\\theta|y) \\propto p(y|\\theta)p(\\theta)\n",
    "$$\n",
    "Let $\\bar{\\theta}^{(s)}$, $s = 1,\\ldots,S$ denote a sample from $p(\\theta|y)$ obtained though application of the MH. With this sample we can _approximate_ various quantities that are of interest to the econometricians.\n",
    "\n",
    "For instance, we may be interested in the posterior mean\n",
    "$$\n",
    "\\mu_\\theta  := \\int \\theta\\, p(\\theta|y) \\, d\\theta\n",
    "$$\n",
    "or the posterior variance\n",
    "$$\n",
    "\\sigma^2_\\theta := \\int \\left(\\theta-\\mu_\\theta\\right)^2p(\\theta|y)\\,d\\theta\n",
    "$$\n",
    "Using $\\bar{\\theta}^{(s)}$ we can approximate $\\mu_\\theta$ and $\\sigma^2_\\theta$ by their sample counterparts\n",
    "$$\n",
    "\\bar{\\mu} = \\frac{1}{S}\\sum_{i=1}^S \\bar{\\theta}^{(s)},\\quad \\bar{\\sigma}^2 = \\frac{1}{S}\\sum_{i=1}^S (\\bar{\\theta}^{(s)}-\\bar{\\mu})^2\n",
    "$$\n",
    "In general, we can approximate any moments of the posterior distribution\n",
    "$$\n",
    "\\int h(\\theta)p(\\theta|y)\\, d\\theta\n",
    "$$\n",
    "by\n",
    "$$\n",
    "\\frac{1}{S}\\sum h(\\bar{\\theta}^{(s)}).\n",
    "$$\n",
    "\n",
    "To apply the MH we need to be able to _evaluate_ $p(y|\\theta)p(\\theta)$. _Evaluate_ means that we can get its value for different values of $\\theta$.\n",
    "\n",
    "## Example: the linear model with Student-t errors\n",
    "\n",
    "To make things concrete, consider the following linear model\n",
    "$$\n",
    "y_t = x_t\\beta + u_t, \\,\\, t = 1,\\ldots,T.\n",
    "$$\n",
    "To accommodate outliers, the errors are assumed to be independently and identically generated by\n",
    "$$\n",
    "u_t \\overset{iid}{\\sim} t(0, \\sigma, \\nu).\n",
    "$$\n",
    "$$\n",
    "f(u) = \\Gamma\\left(\\frac{\\nu+1}{2}\\right)\\left\\{\\Gamma\\left(\\frac{1}{2}\\right)\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sigma \\right\\}^{-1}\\left[1+(u-\\mu)^2/\\nu\\sigma^2\\right]^{-(\\nu+1)/2}\n",
    "$$\n",
    "\n",
    "The prior density is taken to be of the form\n",
    "$$\n",
    "p(\\beta, \\sigma) = p(\\beta)p(\\sigma),\n",
    "$$\n",
    "with improper marginal densities, i.e.\n",
    "$$\n",
    "p(\\beta) \\propto constant,\\,p(\\sigma) = \\sigma^{-1}.\n",
    "$$\n",
    "Although $\\nu$ is part of the specification of the model we will assume that $\\nu$ is known.\n",
    "\n",
    "Given the prior $p(\\beta,\\sigma) \\propto \\sigma^{-1}$ the posterior distribution has _kernel_\n",
    "$$\n",
    "p(\\beta,\\sigma|y,x) = \\sigma^{-(T+1)}\\prod_{t=1}^T \\left[1 + (y_t - x_t\\beta)^2/\\nu\\sigma^2 \\right]^{-(\\nu+1)/2} \n",
    "$$\n",
    "\n",
    "## Generating data \n",
    "\n",
    "We generate fake data from the model above. To do this is useful to introduce the package `Distributions.jl`. This package makes working probability distributions very straightforward. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Distributions\n",
    "t = TDist(3)\n",
    "println(\"The mean of `t` is \", mean(t), \", its variance is \", var(t)) \n",
    "## Generate a random sample from t\n",
    "u = rand(t, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to generate a sample from a Student-t with mean $\\mu$ and variance $\\sigma^2$ we can do as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu = 5\n",
    "sigma2 = 2\n",
    "u = sqrt(sigma2)*rand(t,100)/sqrt(3) + mu;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function generate data from the linear model specified above taking as argument the size of the sample $T$, the number of regressor `k`, the degreees-of-freedom `nu`, the variance of the residual `sigma`, and the \"true\" value of the parametere vector `beta` which default to a zero vector of length `k`. (The regressors are generated from a standard normal distribution). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function simulate(T, k, nu, sigma, beta = zeros(k+1))\n",
    "    # Generate \"fake\" linear model data\n",
    "    t = TDist(nu)\n",
    "    u = sigma*rand(t, T)/sqrt(nu)\n",
    "    X = [ones(T,1) randn(T, k)]\n",
    "    y =  X*beta + u\n",
    "    y, X\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a draw of length $T=100$, with $\\nu=3$ degrees-of-freedom from a model with three regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y, X = simulate(100, 3, 3, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The frequentist way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OLS estimates of the coefficient can be obtained by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta = X\\y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An estimate of the variance of the coefficients is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T, k = size(X)\n",
    "uhat = y-X*beta\n",
    "s2 = dot(uhat, uhat)/(length(uhat)-size(X,2))\n",
    "varb = s2*inv(X'X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard errors are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stderr = diag(varb).^.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the estimated coefficient and the standard errors, a 95% confidence intervals based on the asymptotic normal distribution the OLS estimator is easily obtained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[beta .- 1.96*stderr beta .+ 1.96*stderr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way to obtain the OLS estimates is to use the `GLM.jl` package. `GLM.jl` is a very well designed package that makes it easy estimating and conducting inference in the context of Generalized Linear Models. The syntax through which a model is specified and estimated is very similar to that of `R`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using GLM\n",
    "using DataFrames\n",
    "df = convert(DataFrame, X[:,2:end])\n",
    "df[:y] = y\n",
    "lm = lm(y~x1+x2+x3, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian way\n",
    "Now we turn to the Bayesian approach to estimation in the linear model with Student-t errors. Since inference is entirely based on the posterior distribution we start by writing a function the returns the log-posterior distribution of the model.  \n",
    "\n",
    "`logposterior` takes 4 arguments. The parameter vector `theta`, the data `y` and `X`, and the degrees-of-freedom parameter. Notice that `theta` contains both the slopes of the linear model, i.e. $\\beta_0$, $\\beta_1$, $\\ldots$,$\\beta_k$ and the standard deviation of the residual parameter $\\sigma$. As such, the dimension of `theta` is $k+1+1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function logposterior(theta, y, X, nu)   \n",
    "    T, k  = size(X)\n",
    "    beta  = theta[1:k]\n",
    "    sigma = theta[k+1]\n",
    "    sigma <= 0 && return -Inf\n",
    "    r = y-X*beta\n",
    "    s = nu*sigma^2\n",
    "    tmp = log(1+(r.^2)/s)\n",
    "    loglik = -(T+1)*log(sigma) -((nu+1)/2)*sum(tmp)\n",
    "    return loglik\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood can be evaluated at different values of $\\theta = (\\beta_0,\\beta_1,\\ldots,\\beta_k, \\sigma)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logposterior([0.1, 0.1, 0.2, 0.2, 0.8], y, X, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logposterior([0.4, 0.3, 0.23, 0.25, 0.84], y, X, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to realize that `logposterior` return `-Inf` when the last element of `theta` --- the standard deviation of the residual --- is equal or smaller than zero. This is necessary because the MH algorithm will invoke `logposterior` for values of `theta` generated by a proposal and there is not guarantee that these proposals have a value of $\\sigma$ strictly larger than zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logposterior([0.4, 0.3, 0.23, 0.25, -0.84], y, X, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MH algorithm\n",
    "We now have almost all the pieces to run the MH algorithm to obtain draws from the posterior distribution. The algorithm is coded in the `metroprw` function. Arguments to this function are the number of draws `n_draw`, the initial value of the chain `theta`, the variance of the proposal distribution `Sigma`, the scalar tuning parameter `tau`, the data `y` and `X`, and the `nu` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function metroprw(n_draw, theta0, Sigma, tau, y, X, nu)\n",
    "    T, k = size(X)\n",
    "    NormDist = MvNormal(zeros(k+1), Sigma)\n",
    "    ## Initialization the \n",
    "    theta      = zeros(k+1, n_draw+1);\n",
    "    gamma_s    = logposterior(theta0, y, X, 3);\n",
    "    theta[:,1] = theta0\n",
    "    for s=1:n_draw\n",
    "        theta_star =  theta[:,s] + tau*rand(NormDist, 1);\n",
    "        gamma_star = logposterior(theta_star, y, X, 3);\n",
    "        r = min(exp(gamma_star-gamma_s),1);\n",
    "        u = rand()\n",
    "        if(u<=r)\n",
    "            theta[:, s+1] = theta_star\n",
    "\t\t          gamma_s = gamma_star\n",
    "        else\n",
    "            theta[:, s+1] = theta[:,s]\n",
    "        end\n",
    "     end\n",
    "\n",
    "     return theta\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize the chain, we will follow the practice of starting from the maximum-a-posterior and using as variance of the proposal the inverse negative hessian of the logposterior evaluated at the maximum. To optimize the `logposterior` we use `Optim.jl` a package that containes different optimizers that are relatively easy to use. In what follows we uze the Broyden–Fletcher–Goldfarb–Shanno (`BFGS`) algorithm. We need to create a closure becuase `optim` expects a function in one argument. Also, it expects a that the function me minimized: since we instead want to maximize `logposterior`, we need to pass the negative logposterior. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Optim\n",
    "t0 = vec([beta' s2])\n",
    "lp(theta) = -logposterior(theta, y, X, 3) \n",
    "res = optimize(lp, t0, BFGS())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Calculus\n",
    "theta0 = Optim.minimizer(res)\n",
    "Sigma = full(Hermitian(inv(Calculus.hessian(lp, theta0))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "smpl = metroprw(1000, theta0, Sigma, 1.0, y, X, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Plots;\n",
    "gr();\n",
    "Plots.histogram(smpl', layout=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "smpl = metroprw(100000, theta0, Sigma, 1.0, y, X, 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Plots.histogram(smpl', layout=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gauge whether the algorithm has converged, we can instect the **traceplot** which is a simple plot of the draws. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Plots.plot(smpl', layout=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating posterior mean, posterior variance, posterior quantiles can be easily done by applying `mean`, `var`, and `quantile` to `smpl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean(smpl, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var(smpl,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mapslices(quantile, smpl, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
