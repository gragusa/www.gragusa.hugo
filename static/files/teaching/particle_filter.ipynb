{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Filter with Julia\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In time series models that have latent variables (i.e. have unobservable variables), particle filters (PF) are used to estimate the path of the latent variables up to time $t$ using only information available at time $t$. This operation is called filtering, and it is what gives the PF its name (or at least, part of it). But PF can also be used to estimate the path of the latent variables using all the data, an operation that is called smoothing. Last, but not least, PF are useful to integrate the unobservable variables out of the joint density of observable and unobservable variables to get the marginal density of the observable variables---the likelihood, which can be used to estimate the unknown parameter of the model by using standard Bayesian computational tools.\n",
    "\n",
    "This Julia notebook describes the PF algorithms and its implementation in Julia. The objective is not to write efficient code, but to provide a pedagogical overview of the PF based on actual code. \n",
    "\n",
    "## Requirement\n",
    "\n",
    "In these notes I am assuming the use of julia-0.4.5. With earlier (or later version) the code below is not guaranteed to excecute without wome twicking. \n",
    "\n",
    "The timings presented are relative to this machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 0.5.1\n",
      "Commit 6445c82 (2017-03-05 13:25 UTC)\n",
      "Platform Info:\n",
      "  OS: macOS (x86_64-apple-darwin13.4.0)\n",
      "  CPU: Intel(R) Core(TM) i7-6820HQ CPU @ 2.70GHz\n",
      "  WORD_SIZE: 64\n",
      "  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Haswell)\n",
      "  LAPACK: libopenblas64_\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-3.7.1 (ORCJIT, broadwell)\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires the following packages to be installed and loadable on your installation of Julia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×2 Array{Any,2}:\n",
       " \"Distributions\"  v\"0.12.2\"\n",
       " \"StatsBase\"      v\"0.13.1\"\n",
       " \"StatsFuns\"      v\"0.4.0\" \n",
       " \"Plots\"          v\"0.10.3\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkgs = [\"Distributions\", \"StatsBase\", \"StatsFuns\", \"Plots\"]\n",
    "ver = [Pkg.installed(u) for u in pkgs];\n",
    "[pkgs ver]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Distributions\n",
    "using StatsBase\n",
    "using StatsFuns\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic model\n",
    "\n",
    "\n",
    "Particle methods assume $\\{X_{t}\\}_{t\\geq1}$ and the observations $\\{Y_{t}\\}_{t\\geq1}$ satisfy the following setup:\n",
    "\n",
    "\n",
    "1. $X_{1},X_{2},\\ldots$ is a first order Markov process such that \n",
    "$$\n",
    "X_{t}|\\left(X_{t-1}=x_{t-1}\\right)\\sim f_{\\theta}(\\cdot|x_{t-1})\n",
    "$$\n",
    "with an initial distribution\n",
    "$$\n",
    "X_{1}\\sim\\mu_{\\theta}(x_{1}).\n",
    "$$\n",
    " \n",
    "2. Given $\\{X_{t}\\}_{t\\geq1}$, the observations $\\{Y_{t}\\}_{t\\geq1}$\n",
    "are statistically independent, and their marginal conditional densities are given\n",
    "$$\n",
    "Y_{t}|\\left(X_{t}=x_{t}\\right)\\sim g_{\\theta}(y_{t}|x_{t}),\n",
    "$$\n",
    "\n",
    "A model with these characteristics is the Stochastic Volatility model. \n",
    "\n",
    "## Stochastic Volatility\n",
    "\n",
    "The stochastic volatility model:\n",
    "\n",
    "\\begin{align*}\n",
    "X_{t} & =\\alpha X_{t-1}+\\sigma V_{t}\\\\\n",
    "Y_{t} & =\\beta\\exp\\left(X_{t}/2\\right)W_{t}\n",
    "\\end{align*}\n",
    "where\n",
    "$$\n",
    "V_{t}\\sim N(0,1),\\quad W_{t}\\sim N(0,1).\n",
    "$$\n",
    "\n",
    "In the notation of the generic model, we have:\n",
    "\n",
    "$$\n",
    "\\theta=(\\alpha,\\sigma,\\beta),\\,\\,\\mu_{\\theta}(x)=N\\left(x;0,\\frac{\\sigma^{2}}{1-\\alpha^{2}}\\right),\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_{\\theta}(x_{t}|x_{t-1})=N(x_{t};\\alpha x_{t-1},\\sigma^{2})\n",
    "$$\n",
    "and\n",
    "$$\n",
    "g_{\\theta}(y_{t}|x_{t})=N\\left(y;0,\\beta^{2}\\exp(x)\\right).\n",
    "$$\n",
    "\n",
    "## Simulating the SV model\n",
    "\n",
    "We start by simulating a series $Y$ from the SV model using as parameters \n",
    "$$\n",
    "\\alpha_0 = 0.91,\\,\\ \\sigma_0 = 1\\,\\, \\beta_0 = 0.5.\n",
    "$$\n",
    "\n",
    "We let $\\theta = [\\alpha, \\sigma, \\beta]$, and $\\theta_0 = [0.91, 1.0, 0.5]$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "function simSV(θ, T=100)\n",
    "    α, σ, β = θ\n",
    "    println(\"Simulating SVM\")\n",
    "    println(\"α = \", α, \", σ = \", σ, \", β = \", β)    \n",
    "    X = Array(Float64, T + 1)\n",
    "    Y = Array(Float64, T)\n",
    "    X[1] = 0.0\n",
    "    for t = 1:T\n",
    "        X[t+1] = α*X[t] + σ*randn()\n",
    "        Y[t]   = β*exp(X[t]/2)*randn()\n",
    "        \n",
    "    end\n",
    "    return Y, X\n",
    "end\n",
    "    \n",
    "θ = [.91, 1.0, 0.5];\n",
    "srand(1)\n",
    "Y, X_true = simSV(θ, 500);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We plot the observations (Y) and the volatility (X). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Plots.plot(X_true[2:end], color = :blue, lab = \"Volatility\")\n",
    "Plots.plot!(Y, color = :black, lab = \"Observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle filter algorithm\n",
    "The generic algorithm can be described as follows.\n",
    "\n",
    "---\n",
    "\n",
    "At time $t=1$\n",
    "\n",
    "- Sample $X_{1}^{i}\\sim q_{1}(x_{1})$\n",
    "- Compute the weights $$w_{1}(X_{1}^{i})=\\frac{\\mu(X_{1}^{i})g(y_{1}|X_{1}^{i})}{q(X_{1}^{i})}$$\n",
    "and $$W_{1}^{i} = \\frac{w_{1}(X_{1}^{i})}{\\sum_{i=1}^N w_{1}(X_{1}^{i})}$$\n",
    "\n",
    "At time $t\\geq2$\n",
    "\n",
    "\n",
    "- Sample $X_{t}^{i}\\sim q_{t}(x_{t}|X_{1:t-1}^{i})$ \n",
    "- Compute the weights\n",
    "$$w_{t}(X_{1:t}^{i})  =w_{t-1}(X_{1:t-1}^{i}){\\alpha_{t}(X_{1:t}^i)}$$\n",
    "where \n",
    "$$\n",
    "\\alpha_{t}(X_{1:t}^{i})=\\frac{g_{\\theta}(y_{t}|X_{t}^{i})f_{\\theta}(X_{t}^{i}|X_{t-1}^{i})}{q_{t}(X_{t}^{i}|X_{1:t-1}^{i})}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "At each $t$, the sum of the weights (over $i$) provides an estimate of $p_{\\theta}(y_t|y_{1:t-1})$:\n",
    "$$\n",
    "\\hat{p}_{\\theta}(y_t|y_{1:t-1}) = \\frac{1}{N}\\sum_{i=1}^N w_{t}(X_{1:t}^{i}).\n",
    "$$\n",
    "\n",
    "Since\n",
    "$$\n",
    "p_{\\theta}(y_{1:T}) = p_{\\theta}(y_1)\\prod_{t=2}^T p_{\\theta}(y_t|y_{1:t-1})\n",
    "$$\n",
    "we have\n",
    "$$\n",
    "\\hat{p}_{\\theta}(y_{1:T}) = \\prod_{t=1}^T \\frac{1}{N}\\sum_{i=1}^N w_{t}(X_{1:t}^{i}).\n",
    "$$\n",
    "Importantly, it can be shown that this estimator of the likelihood function is unbiased, that is, \n",
    "$$\n",
    "E\\left[\\hat{p}_{\\theta}(y_{1:T})\\right] = p_{\\theta}(y_{1:T}).\n",
    "$$\n",
    "\n",
    "The proposal distribution, generically denoted $q_{t}(X_{t}^{i}|X_{1:t-1}^{i})$ in the algorithm above, needs to be choosen to make the algorithm feasible. A common choice is to set\n",
    "$$\n",
    "q_{t}(X_{t}^{i}|X_{1:t-1}^{i}) = f_{\\theta}(X_{t}|x_{t-1}).\n",
    "$$\n",
    "With this choice\n",
    "$$\n",
    "\\alpha_{t}(X_{1:t}) = g_{\\theta}(y_t|x_t).\n",
    "$$\n",
    "\n",
    "The function `SIS` below implement the PF just described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function SIS(θ, Y, N=1000)\n",
    "    T = size(Y,1)\n",
    "    X = Array(Float64, N, T) ## This will store the particles\n",
    "    W = Array(Float64, N, T) ## This will store the weights\n",
    "    llik = 0.0\n",
    "    α, σ, β = θ\n",
    "    ## Initialize -- Draws particles\n",
    "    rand!(Normal(0, sqrt(σ^2/(1-α^2))), view(X, :, 1)) ## X_1 ~ μ\n",
    "    sw = 0.0\n",
    "    @inbounds for i = 1:N\n",
    "        W[i, 1] = pdf(Normal(0, β*exp(X[i]/2)), Y[1]) ## W_1 = g(y|X_i)\n",
    "        sw += W[i,1]\n",
    "    end\n",
    "    llik += log(sw)-log(N)\n",
    "    for j = 2:T\n",
    "        sw = 0.0        \n",
    "        @inbounds for i = 1:N\n",
    "            ## Generate particles\n",
    "            X[i,j] = rand(Normal(α*X[i, j-1], σ))\n",
    "            ## Update weights\n",
    "            W[i,j] = W[i,j-1]*pdf(Normal(0, β*exp(X[i,j]/2)), Y[j])\n",
    "            ## sw will contain sum of the weights\n",
    "            sw += W[i,j] \n",
    "        end\n",
    "        llik += log(sw)-log(N)\n",
    "    end    \n",
    "    (scale!(W, vec(1./sum(W, 1))), X, llik)\n",
    "end\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes three arguments: $\\theta$, $Y$, and $N$. The first argument is the parameter vector. The second is the vector of data. The last is the number of particles to be used.\n",
    "\n",
    "The algorithm is relatively fast---even if it is written with readibility rather than efficiency in mind. With $N=1000$, `SIS` takes less than 0.1 second to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W, X, llik = SIS([0.9, 1., .5], Y, 1000);\n",
    "@time W, X, llik = SIS([0.9, 1., .5], Y, 1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the quality of the estimate of the volatility, we plot the \"true\" value of the volatility (that we have here because we are generating the data) and the estimates produced by the PF. As estimates, we use the weighted mean of the particles at each time $t$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_est = Array(Float64, size(X,2))\n",
    "V_est = Array(Float64, size(X,2))\n",
    "for t = 1:size(X,2)\n",
    "    X_est[t] = mean(X[:,t], weights(W[:,t]))\n",
    "    V_est[t] = var(X[:,t], weights(W[:,t]))\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that both  `X_est` and `W_est` are $T\\times N$ matrices. The column $t$ of $W_est$ contains the particles at time $t$ while `W_est` contains the time $t$ weights. This means that `X_est[t] = mean(X[:,t], weights(W[:,t]))` estimates the mean of the volatility at time $t$ given $y_1,\\ldots,y_t$, or\n",
    "$$\n",
    "\\bar{x}_{t|1:t} = \\int x_t \\hat{p}_{\\theta}(x_t|y_{1:t}) d x_t = \\sum_{i=1}^N x_{t}^i W_t^i.\n",
    "$$\n",
    "\n",
    "Similarly, `V_est[t] = var(X[:,t], weights(W[:,t]))` calculates\n",
    "$$\n",
    "\\int (x_t - \\bar{x}_{t|1:t})^2 \\hat{p}_{\\theta}(x_t|y_{1:t}) d x_t = \\sum_{i=1}^N (x_{t}^i-\\bar{x}_{t|1:t})^2 W_t^i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we plot both the true and the filtered volatility and the credibility bands for $t=1,\\ldots,100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Plots.plot(X_true[1:100], color = :red, lab = \"Volatility\")\n",
    "Plots.plot!(X_est[1:100], color = :blue, lab = \"Estimated volatility\")\n",
    "Plots.plot!(X_est[1:100] + sqrt(V_est[1:100]), line=:dot, color = :blue, lab = \"+/- 1 SD\")\n",
    "Plots.plot!(X_est[1:100] - sqrt(V_est[1:100]), line=:dot, color = :blue, lab = \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of the filter is good initially, but it degrades quickly. For $t=50$ onward, the filtered states fail to track the true volatility, and the bands are narrow. Eventually, the filter collapses for larger values of $t$. The reason for the bad performance is the depletion of the particles. The graph below plots the histograms of the particles at $t=1$, $t=5$, $t=20$ and $t=50$. The histograms show that already at $t=20$ only a few particles receive positive weight while at $t=50$ there is only one active particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Plots.plot(W[:,1];  t = :histogram, title = \"t = 1\", lab = \"\", layout = 4)\n",
    "Plots.plot!(W[:,5],  t = :histogram, title = \"t = 5\", lab = \"\", subplot = 2)\n",
    "Plots.plot!(W[:,20], t = :histogram, title = \"t = 20\", lab = \"\", subplot = 3)\n",
    "Plots.plot!(W[:,50], t = :histogram, title = \"t = 50\", lab = \"\", subplot = 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At $t$ grows, eventually only one particle survives, and the log-likelihood becomes $-\\infty$. \n",
    "\n",
    "\n",
    "A way to obviate to the depletion problem is to add a resampling step in the filter. A description of the algorithm with the resempling step is below.\n",
    "\n",
    "---\n",
    "\n",
    "At time $t=1$\n",
    "\n",
    "- Sample $X_{1}^{i}\\sim q_{1}(x_{1})$\n",
    "- Compute the weights $$w_{1}(X_{1}^{i})=\\frac{\\mu(X_{1}^{i})g(y_{1}|X_{1}^{i})}{q(X_{1}^{i}|y_{1})}$$\n",
    "and $$W_{1}^i = \\frac{w_{1}(X_1^i)}{\\sum_{i=1}^N w_{1}(X_1^i)}$$\n",
    "- Resample $X_{1}^{i}$ with probabilities $W_{1}^{i}$. Denote the resampled particle $\\bar{X}_{1}^i$.\n",
    "\n",
    "At time $t\\geq2$\n",
    "\n",
    "\n",
    "- Sample $X_{t}^{i}\\sim q_{t}(x_{t}|\\bar{X}_{1:t-1}^{i})$ and set $X_{1:t}^i = (\\bar{X}_{1:t-1}^i, X_t^i)$\n",
    "- Compute the weights $w_{t}(X_{1:t}^i)  = \\alpha_t(X_{1:t}^i)$ and $W_t^i = \\frac{w_{t}(X_{1:t}^i)}{\\sum_{i=1}^N w_{t}(X_{1:t}^i)}$\n",
    "- Resample $X_{1:t}^i$ with probabilities $W_t^i$ to obtain $\\bar{X}^i_{1:t}$.\n",
    "---\n",
    "\n",
    "\n",
    "There many ways to carry-out the resampling step. We use here the systematic resampling algorithm. \n",
    "\n",
    "The function `SIR` implements the algorithm just described. The function returns two output: the particles, `X`, and the estimated log-likelihood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Performs the systemic resampling algorithm used by particle filters.\n",
    "Parameters\n",
    "----------\n",
    "indexes   : N x 1 Array{Int}\n",
    "            array of indexes into the weights defining the resample.\n",
    "            i.e. the index of the first resample is indexes[1], etc.\n",
    "weights   : N x 1 Array{Float64, 1} weights (sum 1)\n",
    "positions : N x 1 Array{Float64, 1} will contain the positions\n",
    "cumsums   : N x 1 Array{Float64, 1} will contain the cumulative sum\n",
    "-------\n",
    "\"\"\"\n",
    "function systematic_resample!(indexes, w, positions, cumsums)\n",
    "    N = length(w)\n",
    "    U1 = rand(1)[1]/N\n",
    "    @simd for j in 1:N\n",
    "        @inbounds positions[j] = U1 + (j-1)/N\n",
    "    end\n",
    "    cumsum!(cumsums, w)\n",
    "    i, j = 1, 1\n",
    "    @inbounds while i <= N\n",
    "        if positions[i] < cumsums[j]\n",
    "            indexes[i] = j\n",
    "            i += 1\n",
    "        else\n",
    "            j += 1\n",
    "        end\n",
    "    end\n",
    "    return indexes\n",
    "end\n",
    "\n",
    "function SIR(θ, Y, N=1000)\n",
    "    T = size(Y,1)\n",
    "    X = Array{Float64}(N, T) ## This will store the particles\n",
    "    w = Array{Float64}(N)\n",
    "    Xc = Array{Float64}(N)\n",
    "    cumsums = Array{Float64}(N)\n",
    "    indexes = Array{Int64}(N)\n",
    "    positions = Array{Float64}(N)\n",
    "    α, σ, β = θ\n",
    "    ## Initialize -- Draws particles\n",
    "    randn!(view(X, :, 1))\n",
    "    X[:,1] .*= sqrt(σ^2/(1-α^2))\n",
    "    @inbounds for i = 1:N\n",
    "        w[i] = normpdf(0, β*exp(X[i]/2), Y[1]) ## W_1 = g(y|X_i)\n",
    "    end\n",
    "    ## Calculate p(y_1)\n",
    "    llik = log(mean(w))\n",
    "    w ./= sum(w)\n",
    "    ## Resample step\n",
    "    systematic_resample!(indexes, w, positions, cumsums)\n",
    "    X[:,1] = X[indexes,1]\n",
    "    @inbounds for j = 2:T\n",
    "        for i in 1:N\n",
    "            X[i,j] = α*X[i, j-1] + randn()*σ\n",
    "            w[i] = normpdf(0, β*exp(X[i, j]/2), Y[j])\n",
    "        end\n",
    "        llik += log(mean(w))\n",
    "        w ./= sum(w)\n",
    "        systematic_resample!(indexes, w, positions, cumsums)\n",
    "        X[:,1:j] = X[indexes, 1:j]\n",
    "    end\n",
    "    (X, llik)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, llik = SIR(θ,Y, 1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the ability of the filter to estimate the state variables $X$, we plot the true states and the average of the particles. We also plot $\\pm 2$ standard errors calculated as the squared root of the variance of the particles. As it can be seen below, the approximation is much better than the filter without the resampling step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_est = mean(X, 1)'\n",
    "V_est = var(X, 1)'\n",
    "Plots.plot(X_true[1:500], color = :red, lab = \"Volatility\")\n",
    "Plots.plot!(X_est[1:500], color = :blue, lab = \"Estimated volatility\")\n",
    "Plots.plot!(X_est[1:500] + 2*sqrt(V_est[1:500]), line=:dot, color = :blue, lab = \"+/- 1 SD\")\n",
    "Plots.plot!(X_est[1:500] - 2*sqrt(V_est[1:500]), line=:dot, color = :blue, lab = \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle MCMC\n",
    "\n",
    "Bluntly put, the pMCMC consist in using the marginal likelihood obtained from the particle filter as the likelihood in a Metropolis-Hastings algorithm to obtain draws from the posterior of $\\theta$\n",
    "$$\n",
    "p(\\theta|y_{1:T}) \\propto p_{\\theta}(Y_{1:T})p(\\theta)\n",
    "$$\n",
    "A key result is that the particle filter estimator of the likelihood $\\hat{p}_{\\theta}(y_{1:T})$ is an unbiased estimator of the actual likelihood, that is, it holds\n",
    "$$\n",
    "E[\\hat{p}_{\\theta}(y_{1:T})] = p_{\\theta}(y_{1:T}).\n",
    "$$\n",
    "\n",
    "The function `SIR` returns (the second argument) the log-likelihood evaluated at a given value of $\\theta$. However, the function is too slow for the purpouse of MC simulations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@time X, llik = SIR(θ, Y, 1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function allocates too much memory because to keep track of all the particles matrices have to used and these in turns allocate lots of memory in the resampling step. Since for the MCMC we do not need the particles as output we can be clever and using arrays. This function is called `llikSIR` and it is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function llikSIR(θ, Y, N = 1000)\n",
    "    T = size(Y,1)\n",
    "    X  = Array{Float64}(N) ## This will store the particles at time t\n",
    "    X′ = Array{Float64}(N) ## This will store the particles at time t′\n",
    "    w = Array{Float64}(N)\n",
    "    cumsums = Array{Float64}(N)\n",
    "    indexes = Array{Int64}(N)\n",
    "    positions = Array{Float64}(N)\n",
    "    α, σ, β = θ\n",
    "    ## Initialize -- Draws particles\n",
    "    rand!(Normal(0, sqrt(σ^2/(1-α^2))), X)\n",
    "    @inbounds for i = 1:N\n",
    "        w[i] = normpdf(0, β*exp(X[i]/2), Y[1]) ## W_1 = g(y|X_i)\n",
    "    end\n",
    "    ## Calculate likelihood\n",
    "    sumw = sum(w)\n",
    "    llik = log(sumw)\n",
    "    w ./= sumw\n",
    "    ## Resample step\n",
    "    systematic_resample!(indexes, w, positions, cumsums)\n",
    "    @inbounds for j in 1:N\n",
    "        X′[j] = X[indexes[j]]\n",
    "    end\n",
    "    @inbounds for j = 2:T\n",
    "        for i in 1:N\n",
    "            X[i] = α*X′[i] + randn()*σ\n",
    "            w[i] = normpdf(0, β*exp(X[i]/2), Y[j])\n",
    "        end\n",
    "        sumw = sum(w)\n",
    "        llik += log(sumw)\n",
    "        w ./= sumw\n",
    "        if sumw > 0\n",
    "            systematic_resample!(indexes, w, positions, cumsums)\n",
    "        else\n",
    "            break\n",
    "        end\n",
    "        for j in 1:N\n",
    "            X′[j] = X[indexes[j]]\n",
    "        end\n",
    "    end\n",
    "    llik - T*log(N)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate of $\\hat{p}_{\\theta}(y_{1:T})$ is relatively fast for this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen belowm this code is 15 times faster than the previous version. At this speed, we can run 1000 iterations of the MCMC in about 70 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "llikSIR([.9, 1., .5], Y, 1000);\n",
    "@time llikSIR([.9, 1., .5], Y, 1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In estimating the parameters we elicit two uniform prior for $\\alpha$ and $\\beta$ and a Gamma prior for $\\sigma$. These priors are relatively uninformative. The MCMC is a random walk metropolis algorithm. As usual we have to set the scale of the proposal distribution and the parameter $\\tau$. The code below implement the MCMC sampler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "πσ = Gamma(2,1)     ## prior on σ\n",
    "πα = Uniform(-1,1)  ## pr ior on α\n",
    "πβ = Uniform(0,1)   ## prior in β\n",
    "\n",
    "## n_particle\n",
    "n_particle = 1000\n",
    "\n",
    "function metroprw(n_draw, theta0, Sigma, tau, Y, n_particle = 1000)\n",
    "    k = length(theta0)\n",
    "    NormDist = MvNormal(zeros(k), Sigma)\n",
    "    ## Initialization\n",
    "    theta      = zeros(k, n_draw+1)\n",
    "    gamma_s    = logposterior(theta0, Y, n_particle)\n",
    "    theta[:,1] = theta0\n",
    "    for s=1:n_draw\n",
    "        theta_star =  theta[:,s] + tau*rand(NormDist, 1)\n",
    "        rem(s, 100) == 0 && println(\"Iteration $s Status: $theta_star Likelihood: $gamma_s\")\n",
    "        gamma_star = logposterior(theta_star, Y, n_particle)\n",
    "        r = min(exp(gamma_star-gamma_s),1)\n",
    "        u = rand()\n",
    "        if(u<=r)\n",
    "            theta[:, s+1] = theta_star\n",
    "\t\t          gamma_s = gamma_star\n",
    "        else\n",
    "            theta[:, s+1] = theta[:,s]\n",
    "        end\n",
    "     end\n",
    "     return theta\n",
    "end\n",
    "\n",
    "function logposterior(theta, Y, n_particle)\n",
    "    α,σ,β = theta\n",
    "    logprior = logpdf(πσ, σ) + logpdf(πβ, β) + logpdf(πα, α)\n",
    "    isfinite(logprior) ? llikSIR(theta, Y, n_particle) + logprior : -Inf     \n",
    "end\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logposterior(θ, Y, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = metroprw(1000, θ, 0.1*eye(3), .3, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Plots.histogram(out', layout = 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
